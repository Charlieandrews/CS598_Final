{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "EDhJ93xXQ63g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8438431-cc95-4bad-fd37-d9dc40a8e6b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyts\n",
            "  Downloading pyts-0.13.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from pyts) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from pyts) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyts) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from pyts) (1.4.2)\n",
            "Requirement already satisfied: numba>=0.55.2 in /usr/local/lib/python3.11/dist-packages (from pyts) (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.55.2->pyts) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.0->pyts) (3.6.0)\n",
            "Downloading pyts-0.13.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyts\n",
            "Successfully installed pyts-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import gdown\n",
        "\n",
        "data_train_id = \"1oG5FtbXgUgnYkQI-Om-MBq_n1s2dYUAt\"\n",
        "data_test_id = \"1dViHrJP7Lm6nr_RhEaDB9A7IlOwy439s\"\n",
        "target_train_id = \"1km9rjrYGDyQlLE_20Q4kLjJXWoXLgEA9\"\n",
        "target_test_id = \"1N-xiJWDqQJq2UMaafuBwGg6ghRLvrpEM\"\n",
        "\n",
        "data_train_norm_id = \"1sstp-w4HBoFiegK90uHiMwuOygNOKaEU\"\n",
        "data_test_norm_id = \"1CyN22bU0iyYbgMwUVYNSJPc_GwqMCL1P\"\n",
        "\n",
        "\n",
        "\n",
        "gdown.download(id=data_train_id, output=\"data_train.npy\", quiet=True)\n",
        "gdown.download(id=data_test_id, output=\"data_test.npy\", quiet=True)\n",
        "gdown.download(id=target_train_id, output=\"target_train.npy\", quiet=True)\n",
        "gdown.download(id=target_test_id, output=\"target_test.npy\", quiet=True)\n",
        "gdown.download(id=data_train_norm_id, output=\"data_train_norm.npy\", quiet=True)\n",
        "gdown.download(id=data_test_norm_id, output=\"data_test_norm.npy\", quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jTeZzlH4NfQQ",
        "outputId": "3553e41f-2b2f-4bf9-fe41-b3ede853168e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data_test_norm.npy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports & Data Loading\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# — Replace these with your actual data-loading steps —\n",
        "# For example, if you have .npy files:\n",
        "data_train_norm = np.load('data_train_norm.npy')\n",
        "data_test_norm  = np.load('data_test_norm.npy')\n",
        "target_train    = np.load('target_train.npy')\n",
        "target_test     = np.load('target_test.npy')\n"
      ],
      "metadata": {
        "id": "LMAKi0ZGNZP7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Model Definitions\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, LSTM, Dense,\n",
        "    Conv1D, BatchNormalization,\n",
        "    GlobalAveragePooling1D\n",
        ")\n",
        "\n",
        "# def build_lstm(input_shape, num_classes):\n",
        "#     inp = Input(shape=input_shape)\n",
        "#     x   = LSTM(64)(inp)\n",
        "#     out = Dense(num_classes, activation='softmax')(x)\n",
        "#     return inp, out\n",
        "\n",
        "# from tensorflow.keras.layers import Input, Dense, Flatten\n",
        "\n",
        "# def build_encoder(input_shape, num_classes):\n",
        "#     inp = Input(shape=input_shape)      # (None, timesteps, 1)\n",
        "#     x   = Flatten()(inp)                # (None, timesteps * 1)\n",
        "#     x   = Dense(64, activation='relu')(x)\n",
        "#     x   = Dense(64, activation='relu')(x)\n",
        "#     out = Dense(num_classes, activation='softmax')(x)\n",
        "#     return inp, out\n",
        "\n",
        "\n",
        "\n",
        "def build_resnet(input_shape, filters, num_classes):\n",
        "    inp = Input(shape=input_shape)\n",
        "    x   = Conv1D(filters, 3, padding='same', activation='relu')(inp)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = Conv1D(filters, 3, padding='same', activation='relu')(x)\n",
        "    x   = BatchNormalization()(x)\n",
        "    x   = GlobalAveragePooling1D()(x)\n",
        "    out = Dense(num_classes, activation='softmax')(x)\n",
        "    return inp, out\n"
      ],
      "metadata": {
        "id": "qoDUyO-WNaSK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Hyperparameters & Scheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "NUM_CLASSES   = int(np.max([target_train.max(), target_test.max()]) + 1)\n",
        "EPOCHS        = 50\n",
        "BATCH_SIZE    = 16\n",
        "LEARNING_RATE = 5e-3\n",
        "source_idx    = 0   # which view to use\n",
        "\n",
        "lr_schedule = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-5,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "build_models = [\n",
        "    # ('lstm',    build_lstm)\n",
        "    # ('encoder', build_encoder)\n",
        "    ('resnet',  build_resnet)\n",
        "]\n"
      ],
      "metadata": {
        "id": "zCpLdYolNiEP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Train & Summarize Baseline Accuracies\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "accuracies = {}\n",
        "\n",
        "for name, build_fn in build_models:\n",
        "    print(f\"\\n--- Training {name} on view{source_idx} ---\")\n",
        "    # reshape to (samples, timesteps, channels=1)\n",
        "    X_tr = data_train_norm[:, :, source_idx].reshape(-1, data_train_norm.shape[1], 1)\n",
        "    X_te = data_test_norm[:,  :, source_idx].reshape(-1, data_test_norm.shape[1], 1)\n",
        "    y_tr = to_categorical(target_train, NUM_CLASSES)\n",
        "    y_te = to_categorical(target_test,  NUM_CLASSES)\n",
        "\n",
        "    # build model\n",
        "    if name == 'resnet':\n",
        "        inp, out = build_fn(X_tr.shape[1:], 32, NUM_CLASSES)\n",
        "    else:\n",
        "        inp, out = build_fn(X_tr.shape[1:], NUM_CLASSES)\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # fit\n",
        "    hist = model.fit(\n",
        "        X_tr, y_tr,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_data=(X_te, y_te),\n",
        "        callbacks=[lr_schedule],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    val_acc = hist.history['val_accuracy'][-1]\n",
        "    accuracies[name] = val_acc\n",
        "    print(f\"{name:7s} final val_accuracy = {val_acc:.4f}\")\n",
        "\n",
        "print(\"\\n=== Baseline (no-transfer) Accuracies ===\")\n",
        "for m, a in accuracies.items():\n",
        "    print(f\" • {m:<7s}: {a:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7Znmf4VNjMf",
        "outputId": "59f0a980-70e5-4752-c5e0-2e1368c00338"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training resnet on view0 ---\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
            "\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
            "\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
            "\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
            "resnet  final val_accuracy = 0.4741\n",
            "\n",
            "=== Baseline (no-transfer) Accuracies ===\n",
            " • resnet : 47.41%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}