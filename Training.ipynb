{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "OQvLuVmlihLZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EDhJ93xXQ63g"
      },
      "outputs": [],
      "source": [
        "!pip install pyts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IjhKn9BwwoC"
      },
      "outputs": [],
      "source": [
        "# Download data from Google Drive\n",
        "import gdown\n",
        "\n",
        "data_train_id = \"1oG5FtbXgUgnYkQI-Om-MBq_n1s2dYUAt\"\n",
        "data_test_id = \"1dViHrJP7Lm6nr_RhEaDB9A7IlOwy439s\"\n",
        "target_train_id = \"1km9rjrYGDyQlLE_20Q4kLjJXWoXLgEA9\"\n",
        "target_test_id = \"1N-xiJWDqQJq2UMaafuBwGg6ghRLvrpEM\"\n",
        "\n",
        "data_train_norm_id = \"1sstp-w4HBoFiegK90uHiMwuOygNOKaEU\"\n",
        "data_test_norm_id = \"1CyN22bU0iyYbgMwUVYNSJPc_GwqMCL1P\"\n",
        "\n",
        "\n",
        "\n",
        "gdown.download(id=data_train_id, output=\"data_train.npy\", quiet=True)\n",
        "gdown.download(id=data_test_id, output=\"data_test.npy\", quiet=True)\n",
        "gdown.download(id=target_train_id, output=\"target_train.npy\", quiet=True)\n",
        "gdown.download(id=target_test_id, output=\"target_test.npy\", quiet=True)\n",
        "gdown.download(id=data_train_norm_id, output=\"data_train_norm.npy\", quiet=True)\n",
        "gdown.download(id=data_test_norm_id, output=\"data_test_norm.npy\", quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bT3h-E0krLBL"
      },
      "outputs": [],
      "source": [
        "# Load data and shuffle\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "data_train = np.load(\"data_train.npy\", allow_pickle=True)\n",
        "data_test = np.load(\"data_test.npy\")\n",
        "target_train = np.load(\"target_train.npy\")\n",
        "target_test = np.load(\"target_test.npy\")\n",
        "data_train_norm = np.load(\"data_train_norm.npy\")\n",
        "data_test_norm = np.load(\"data_test_norm.npy\")\n",
        "\n",
        "data_train, target_train, data_train_norm = shuffle(data_train, target_train, data_train_norm, random_state=42)\n",
        "data_test, target_test, data_test_norm = shuffle(data_test, target_test, data_test_norm, random_state=42)\n",
        "\n",
        "NUM_CLASSES = 19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wEVfnYdR1jcU"
      },
      "outputs": [],
      "source": [
        "# Build models as described in the original paper\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import sklearn.metrics\n",
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from pyts.metrics import boss, dtw\n",
        "\n",
        "def build_mlp(input_shape,nb_classes):\n",
        "    x = keras.layers.Input(shape=(input_shape))\n",
        "    #a Layer instance is callable on a tensor , and returns a tensor\n",
        "    x_nb = Dense(64 , activation='relu')(x)\n",
        "    x_nb = keras.layers.Dropout(0.2)(x_nb)\n",
        "    out = Dense(nb_classes,activation='softmax')(x_nb)\n",
        "    return x, out\n",
        "\n",
        "def build_lstm(input_shape,nb_classes):\n",
        "    x = keras.layers.Input(shape=input_shape)\n",
        "    x_nb = LSTM(64)(x)\n",
        "    x_nb = keras.layers.Dropout(0.2)(x_nb)\n",
        "    out = Dense(nb_classes,activation='softmax')(x_nb)\n",
        "    return x, out\n",
        "\n",
        "def build_fcn(input_shape,nb_classes):\n",
        "    x = keras.layers.Input(shape=(input_shape))\n",
        "    conv_x = keras.layers.BatchNormalization()(x)\n",
        "    conv_x = keras.layers.Conv1D(128, kernel_size=8, padding='same')(conv_x)\n",
        "    # conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "    conv_x = keras.layers.Dropout(0.2)(conv_x)\n",
        "    full = keras.layers.GlobalAveragePooling1D()(conv_x)\n",
        "    out = keras.layers.Dense(nb_classes, activation='softmax')(full)\n",
        "    return x, out\n",
        "def build_resnet(input_shape, n_feature_maps, nb_classes):\n",
        "    #print('build conv_x')\n",
        "    x = keras.layers.Input(shape=(input_shape))\n",
        "    conv_x = keras.layers.BatchNormalization()(x)\n",
        "    conv_x = keras.layers.Conv1D(n_feature_maps, kernel_size=8, padding='same')(conv_x)\n",
        "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "    #print('build conv_y')\n",
        "    conv_y = keras.layers.Conv1D(n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
        "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "    #print('build conv_z')\n",
        "    conv_z = keras.layers.Conv1D(n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
        "    if is_expand_channels:\n",
        "        shortcut_y = keras.layers.Conv1D(n_feature_maps, kernel_size=1, padding='same')(x)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "    else:\n",
        "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
        "    #print('Merging skip connection')\n",
        "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
        "    y = keras.layers.Activation('relu')(y)\n",
        "\n",
        "    #print('build conv_x')\n",
        "    x1 = y\n",
        "    conv_x = keras.layers.Conv1D(n_feature_maps * 2, kernel_size=8, padding='same')(x1)\n",
        "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "    #print('build conv_y')\n",
        "    conv_y = keras.layers.Conv1D(n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "    #print('build conv_z')\n",
        "    conv_z = keras.layers.Conv1D(n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "    is_expand_channels = not (input_shape[-1] == n_feature_maps * 2)\n",
        "    if is_expand_channels:\n",
        "        shortcut_y = keras.layers.Conv1D(n_feature_maps * 2, kernel_size=1, padding='same')(x1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "    else:\n",
        "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
        "    #print('Merging skip connection')\n",
        "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
        "    y = keras.layers.Activation('relu')(y)\n",
        "\n",
        "    #print('build conv_x')\n",
        "    x1 = y\n",
        "    conv_x = keras.layers.Conv1D(n_feature_maps * 2, kernel_size=8, padding='same')(x1)\n",
        "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "    #print('build conv_y')\n",
        "    conv_y = keras.layers.Conv1D(n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "    #print('build conv_z')\n",
        "    conv_z = keras.layers.Conv1D(n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "    is_expand_channels = not (input_shape[-1] == n_feature_maps * 2)\n",
        "    if is_expand_channels:\n",
        "        shortcut_y = keras.layers.Conv1D(n_feature_maps * 2, kernel_size=1, padding='same')(x1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "    else:\n",
        "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
        "    #print('Merging skip connection')\n",
        "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
        "    y = keras.layers.Activation('relu')(y)\n",
        "\n",
        "    full = keras.layers.GlobalAveragePooling1D()(y)\n",
        "    out = keras.layers.Dense(nb_classes, activation='softmax')(full)\n",
        "    #print('        -- model was built.')\n",
        "    return x, out\n",
        "\n",
        "\n",
        "def build_encoder(input_shape, nb_classes):\n",
        "    x = keras.layers.Input(input_shape)\n",
        "\n",
        "    # conv block -1\n",
        "    conv1 = keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='same')(x)\n",
        "    conv1 = keras.layers.GroupNormalization()(conv1)\n",
        "    # conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
        "    conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
        "    conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
        "    conv1 = keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
        "    # conv block -2\n",
        "    conv2 = keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='same')(conv1)\n",
        "    conv2 = keras.layers.GroupNormalization()(conv2)\n",
        "    # conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
        "    conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
        "    conv2 = keras.layers.Dropout(rate=0.2)(conv2)\n",
        "    conv2 = keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
        "    # conv block -3\n",
        "    conv3 = keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='same')(conv2)\n",
        "    conv3 = keras.layers.GroupNormalization()(conv3)\n",
        "    # conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
        "    conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
        "    conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n",
        "    # split for attention\n",
        "    attention_data = keras.layers.Lambda(lambda x: x[:,:,:256])(conv3)\n",
        "    attention_softmax = keras.layers.Lambda(lambda x: x[:,:,256:])(conv3)\n",
        "    # attention mechanism\n",
        "    attention_softmax = keras.layers.Softmax()(attention_softmax)\n",
        "    multiply_layer = keras.layers.Multiply()([attention_softmax,attention_data])\n",
        "    # last layer\n",
        "    dense_layer = keras.layers.Dense(units=256,activation='sigmoid')(multiply_layer)\n",
        "    dense_layer = keras.layers.GroupNormalization()(dense_layer)\n",
        "    # dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
        "    # output layer\n",
        "    flatten_layer = keras.layers.Flatten()(dense_layer)\n",
        "    out = keras.layers.Dense(units=nb_classes,activation='softmax')(flatten_layer)\n",
        "\n",
        "    return x,out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pJrn0EzgI-cA"
      },
      "outputs": [],
      "source": [
        "def dtw_classic(x, y):\n",
        "    return dtw(x, y, method='classic')\n",
        "def dtw_sakoechiba(x, y, window_size):\n",
        "    return dtw(x, y, method='sakoechiba', options={'window_size': window_size})\n",
        "def dtw_itakura(x, y, max_slope):\n",
        "    return dtw(x, y, method='itakura', options={'max_slope': max_slope})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YOF1whiF2Uy2"
      },
      "outputs": [],
      "source": [
        "def cal_similarity(view1,view2,metric='boss'):\n",
        "    similarity_list = []\n",
        "    for i in range(view1.shape[0]):\n",
        "        if metric == 'boss':\n",
        "            similarity_list.append(boss(np.squeeze(view1[i]),np.squeeze(view2[i])))\n",
        "        elif metric == 'dtw_classic':\n",
        "            similarity_list.append(dtw_classic(np.squeeze(view1[i]),np.squeeze(view2[i])))\n",
        "        elif metric == 'dtw_sakoechiba':\n",
        "            similarity_list.append(dtw_sakoechiba(np.squeeze(view1[i]),np.squeeze(view2[i]),window_size=0.5))\n",
        "        elif metric == 'dtw_itakura':\n",
        "            similarity_list.append(dtw_itakura(np.squeeze(view1[i]),np.squeeze(view2[i]), max_slope=1.5))\n",
        "        # elif metric == 'dtw_multiscale':\n",
        "        #     similarity_list.append(dtw_multiscale(np.squeeze(view1[i]),np.squeeze(view2[i]), resolution=2) )\n",
        "        # elif metric == 'dtw_fast':\n",
        "        #     similarity_list.append(dtw_fast(np.squeeze(view1[i]),np.squeeze(view2[i]),radius = 1))\n",
        "        else:\n",
        "            print('other metric not implement yet.')\n",
        "    return np.array(similarity_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# No Transfer Learning"
      ],
      "metadata": {
        "id": "NNGMS4NKb_Sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_idx=0\n",
        "\n",
        "lr_schedule = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,           # reduce by half\n",
        "    patience=5,           # wait 5 epochs with no improvement\n",
        "    min_lr=1e-5,          # don't go below this\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "build_models = [('lstm', build_lstm), ('encoder', build_encoder), ('resnet', build_resnet)]\n",
        "\n",
        "\n",
        "for build_model in build_models:\n",
        "  model_name, build_fn = build_model\n",
        "\n",
        "  for i in range(int(data_train.shape[2])):\n",
        "    view_name = 'view' + str(i)\n",
        "    # locals()[view_name+'_train'] = data_train[:,:,i].reshape(data_train.shape[0],data_train.shape[1],1)\n",
        "    locals()[view_name+'_train'] = data_train_norm[:,:,i].reshape(data_train.shape[0],125,9)\n",
        "\n",
        "    # locals()[view_name+'_test'] = data_test[:,:,i].reshape(data_test.shape[0],data_test.shape[1],1)\n",
        "    locals()[view_name+'_test'] = data_test_norm[:,:,i].reshape(data_test.shape[0],125,9)\n",
        "\n",
        "  print(f'********************* No transfer learning: {model_name} ********************')\n",
        "\n",
        "  x, y = None, None\n",
        "  if model_name == 'resnet':\n",
        "    x, y = build_fn(locals()[\"view0_train\"].shape[1:], 32, NUM_CLASSES)\n",
        "  else:\n",
        "    x, y = build_fn(locals()[\"view0_train\"].shape[1:], NUM_CLASSES)\n",
        "\n",
        "\n",
        "  model = keras.models.Model(inputs=x, outputs=y)\n",
        "  adam = Adam(learning_rate=0.005)\n",
        "  # chk = ModelCheckpoint('best_model.pkl', monitor=tf.keras.metrics.Accuracy(), save_best_only=True, mode='max', verbose=1)\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.25)\n",
        "  # model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "  model.compile(loss=loss, optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "  history=model.fit(locals()['view'+str(source_idx)+'_train'],\n",
        "              to_categorical(target_train, num_classes=NUM_CLASSES),\n",
        "              epochs=50,\n",
        "              batch_size=16,\n",
        "              validation_data=(locals()['view'+str(source_idx)+'_test'],to_categorical(target_test, num_classes=NUM_CLASSES)),\n",
        "              callbacks=lr_schedule,\n",
        "              verbose=0)\n",
        "\n",
        "\n",
        "  print(history.history['val_accuracy'][-1])\n",
        "\n",
        "  model.save(f'no_transfer_model_{model_name}.h5')\n",
        "  model.save(f'no_transfer_model_{model_name}.keras')\n",
        "  with open(f'no_transfer_history_{model_name}.pkl', 'wb') as f:\n",
        "      pickle.dump(history, f)"
      ],
      "metadata": {
        "id": "KFZwZP8mcEoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Transfer Learning"
      ],
      "metadata": {
        "id": "dPPY1WvocCxM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQTvj5id2Xjm"
      },
      "outputs": [],
      "source": [
        "source_idx=0\n",
        "\n",
        "build_models = [('lstm', build_lstm), ('encoder', build_encoder), ('resnet', build_resnet)]\n",
        "\n",
        "\n",
        "for model_name, build_fn in build_models:\n",
        "  for i in range(int(data_train.shape[2])):\n",
        "    view_name = 'view' + str(i)\n",
        "    # locals()[view_name+'_train'] = data_train[:,:,i].reshape(data_train.shape[0],data_train.shape[1],1)\n",
        "    locals()[view_name+'_train'] = data_train_norm[:,:,i].reshape(data_train.shape[0],125,9)\n",
        "\n",
        "    # locals()[view_name+'_test'] = data_test[:,:,i].reshape(data_test.shape[0],data_test.shape[1],1)\n",
        "    locals()[view_name+'_test'] = data_test_norm[:,:,i].reshape(data_test.shape[0],125,9)\n",
        "\n",
        "  print(f'********************* Naive transfer learning {model_name} ********************')\n",
        "  x, y = None, None\n",
        "  if model_name == 'resnet':\n",
        "    x, y = build_fn(locals()[\"view0_train\"].shape[1:], 32, NUM_CLASSES)\n",
        "  else:\n",
        "    x, y = build_fn(locals()[\"view0_train\"].shape[1:], NUM_CLASSES)\n",
        "\n",
        "  model = keras.models.Model(inputs=x, outputs=y)\n",
        "  adam = Adam(learning_rate=0.005)\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.25)\n",
        "  # model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "  model.compile(loss=loss, optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "  # transfer learning\n",
        "  for i in range(int(data_train.shape[2])):\n",
        "    print(f'Source idx: {i}')\n",
        "    if i != source_idx:\n",
        "        view_name = 'view' + str(i)\n",
        "        weight_name = 'weight' + str(source_idx) + str(i)\n",
        "        model.fit(locals()[view_name+'_train'],\n",
        "                  to_categorical(target_train),\n",
        "                  epochs=30,\n",
        "                  batch_size=16,\n",
        "                  validation_data=(locals()[view_name+'_test'],to_categorical(target_test)),\n",
        "                  verbose=0)\n",
        "  # on target domain\n",
        "  history=model.fit(locals()['view'+str(source_idx)+'_train'],\n",
        "              to_categorical(target_train),\n",
        "              epochs=40,\n",
        "              batch_size=16,\n",
        "              validation_data=(locals()['view'+str(source_idx)+'_test'], to_categorical(target_test)),\n",
        "              verbose=0)\n",
        "\n",
        "  print(history.history['val_accuracy'][-1])\n",
        "\n",
        "  model.save(f'naive_transfer_model_{model_name}.h5')\n",
        "  model.save(f'naive_transfer_model_{model_name}.keras')\n",
        "  with open(f'naive_transfer_history_{model_name}.pkl', 'wb') as f:\n",
        "      pickle.dump(history, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weighted Transfer Learning"
      ],
      "metadata": {
        "id": "i6Es_pr3ASg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "source_idx=0\n",
        "\n",
        "build_models = [('lstm', build_lstm), ('encoder', build_encoder), ('resnet', build_resnet)]\n",
        "\n",
        "# for metric in ['boss','dtw_classic','dtw_sakoechiba','dtw_itakura']:\n",
        "for metric in ['dtw_classic']:\n",
        "  mode_name=metric+'+kde'\n",
        "  for i in range(int(data_train.shape[2])):\n",
        "      view_name = 'view' + str(i)\n",
        "      locals()[view_name+'_train'] = data_train_norm[:,:,i].reshape(data_train.shape[0],data_train.shape[1],1)\n",
        "      locals()[view_name+'_test'] = data_test_norm[:,:,i].reshape(data_test.shape[0],data_test.shape[1],1)\n",
        "\n",
        "  for i in range(int(data_train.shape[2])):\n",
        "          similarity_name = 'similarity' + str(source_idx)\n",
        "          print(i)\n",
        "          if i != source_idx:\n",
        "              locals()[similarity_name+str(i)] = cal_similarity(locals()['view'+str(source_idx)+'_train'],locals()['view'+str(i)+'_train'],metric)\n",
        "      # # KDE\n",
        "  for i in range(int(data_train.shape[2])):\n",
        "          kde_name = 'kde'+ str(source_idx)\n",
        "          similarity_name = 'similarity' + str(source_idx)\n",
        "          if i != source_idx:\n",
        "              locals()[kde_name+str(i)] = KernelDensity(kernel='gaussian', bandwidth=7.8).fit(locals()[similarity_name+str(i)].reshape(locals()[similarity_name+str(i)].flatten().shape[0],1))\n",
        "  # weight\n",
        "  weight_all = 0\n",
        "  for i in range(int(data_train.shape[2])):\n",
        "          if i != source_idx:\n",
        "              kde_name = 'kde'+ str(source_idx) + str(i)\n",
        "              weight_name = 'weight' + str(source_idx) + str(i)\n",
        "              locals()[weight_name] =  np.mean(locals()[kde_name].sample(10,random_state=0),axis=0)[0]\n",
        "              weight_all += locals()[weight_name]\n",
        "  for i in range(int(data_train.shape[2])):\n",
        "    view_name = 'view' + str(i)\n",
        "    # locals()[view_name+'_train'] = data_train[:,:,i].reshape(data_train.shape[0],data_train.shape[1],1)\n",
        "    locals()[view_name+'_train'] = data_train_norm[:,:,i].reshape(data_train.shape[0],125,9)\n",
        "\n",
        "    # locals()[view_name+'_test'] = data_test[:,:,i].reshape(data_test.shape[0],data_test.shape[1],1)\n",
        "    locals()[view_name+'_test'] = data_test_norm[:,:,i].reshape(data_test.shape[0],125,9)\n",
        "\n",
        "\n",
        "  for model_name, build_fn in build_models:\n",
        "\n",
        "      print(f'********************* Weighted transfer learning {model_name} ********************')\n",
        "\n",
        "      x, y = None, None\n",
        "      if model_name == 'resnet':\n",
        "        x, y = build_fn(locals()[\"view0_train\"].shape[1:], 32, NUM_CLASSES)\n",
        "      else:\n",
        "        x, y = build_fn(locals()[\"view0_train\"].shape[1:], NUM_CLASSES)\n",
        "\n",
        "      model = keras.models.Model(inputs=x, outputs=y)\n",
        "      adam = Adam(learning_rate=0.0005)\n",
        "      loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.25)\n",
        "      # model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "      model.compile(loss=loss, optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "      # transfer learning\n",
        "      for i in range(int(data_train.shape[2])):\n",
        "              if i != source_idx:\n",
        "                  view_name = 'view' + str(i)\n",
        "                  weight_name = 'weight' + str(source_idx) +str(i)\n",
        "                  model.fit(locals()[view_name+'_train'], to_categorical(target_train), epochs=int(30*7*locals()[weight_name]/weight_all)+1,batch_size=16,validation_data=(locals()[view_name+'_test'],to_categorical(target_test)),verbose=1)\n",
        "      # on target domain\n",
        "      history=model.fit(locals()['view'+str(source_idx)+'_train'],\n",
        "                  to_categorical(target_train),\n",
        "                  epochs=40,\n",
        "                  batch_size=16,\n",
        "                  validation_data=(locals()['view'+str(source_idx)+'_test'],to_categorical(target_test)),verbose=1)\n",
        "\n",
        "      model.save(f'weighted_transfer_model_{model_name}.h5')\n",
        "      model.save(f'weighted_transfer_model_{model_name}.keras')\n",
        "      with open(f'weighted_transfer_history_{model_name}.pkl', 'wb') as f:\n",
        "          pickle.dump(history, f)"
      ],
      "metadata": {
        "id": "4_DlQSLmAcGA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}